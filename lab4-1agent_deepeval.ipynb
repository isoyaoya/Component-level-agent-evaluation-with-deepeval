{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "# Using DeepEval for AWS Bedrock LLM evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build an agent with AWS Strands Agent\n",
    "\n",
    "Strands Agents is a powerful framework for building AI agents that can interact with AWS services and perform complex tasks. We will quick create the Strands agent first.\n",
    "\n",
    "**Prerequisites**\n",
    "\n",
    "- Python 3.10 or later\n",
    "- AWS account configured with appropriate permissions\n",
    "- Basic understanding of Python programming\n",
    "\n",
    "Lets get started !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install strands-agents strands-agents-tools boto3 botocore -Uqqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Restart kernel (only works on Linux)\n",
    "import os\n",
    "\n",
    "os._exit(00)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set Your AWS Credentials**\n",
    "There are multiple ways to set your AWS Credentials depending on your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.Session(\n",
    "    region_name=\"us-east-1\",\n",
    "    aws_access_key_id=\"<YOUR_ACCESS_KEY_ID>\",\n",
    "    aws_secret_access_key=\"<YOUR_SECRET_ACCESS_KEY>\",\n",
    "    aws_session_token=\"<YOUR_SESSION_TOKEN>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Custom Tool Demonstration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from strands import Agent, tool\n",
    "from strands_tools import calculator, current_time, python_repl,file_read,shell,file_write\n",
    "import nest_asyncio\n",
    "\n",
    "# Apply nest_asyncio at the start\n",
    "nest_asyncio.apply()\n",
    "\n",
    "@tool\n",
    "def word_count(text: str) -> int:\n",
    "    \"\"\"Count words in text.\n",
    "\n",
    "    This docstring is used by the LLM to understand the tool's purpose.\n",
    "    \"\"\"\n",
    "    return len(text.split())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define an agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent = Agent(tools=[calculator, current_time, python_repl, word_count,file_read,shell,file_write],model=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'll help you read the letter.txt file and count the total number of words.\n",
      "Tool #3: file_read\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">‚ïî‚ïê‚ïê‚ïê‚ïê </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">üìÑ letter.txt</span><span style=\"color: #000080; text-decoration-color: #000080\"> ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">‚ïë</span>                        <span style=\"color: #000080; text-decoration-color: #000080\">‚ïë</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">‚ïë</span>  <span style=\"color: #e3e3dd; text-decoration-color: #e3e3dd; background-color: #272822; font-weight: bold\">  </span><span style=\"color: #656660; text-decoration-color: #656660; background-color: #272822\">1 </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">YOU ARE THE BEST</span>  <span style=\"color: #000080; text-decoration-color: #000080\">‚ïë</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">‚ïë</span>                        <span style=\"color: #000080; text-decoration-color: #000080\">‚ïë</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34m‚ïî‚ïê\u001b[0m\u001b[34m‚ïê‚ïê‚ïê\u001b[0m\u001b[34m \u001b[0m\u001b[1;32müìÑ letter.txt\u001b[0m\u001b[34m \u001b[0m\u001b[34m‚ïê‚ïê‚ïê‚ïê\u001b[0m\u001b[34m‚ïê‚ïó\u001b[0m\n",
       "\u001b[34m‚ïë\u001b[0m                        \u001b[34m‚ïë\u001b[0m\n",
       "\u001b[34m‚ïë\u001b[0m  \u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m1 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mYOU ARE THE BEST\u001b[0m  \u001b[34m‚ïë\u001b[0m\n",
       "\u001b[34m‚ïë\u001b[0m                        \u001b[34m‚ïë\u001b[0m\n",
       "\u001b[34m‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tool #4: word_count\n",
      "The letter.txt file contains 4 words."
     ]
    }
   ],
   "source": [
    "message = \"You are a helpful assistant that provides concise responses. Help me to read letter.txt file, count the total number of words.\"\n",
    "\n",
    "results = agent(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**See the excution and tool use results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool Use:\n",
      "\tToolUseId:  tooluse_sN51vJtsQGCD-_w1LwTIYw\n",
      "\tname:  file_read\n",
      "\tinput:  {'path': 'letter.txt', 'mode': 'view'}\n",
      "Tool Result:\n",
      "\tToolUseId:  tooluse_sN51vJtsQGCD-_w1LwTIYw\n",
      "\tStatus:  success\n",
      "\tContent:  [{'text': 'Content of letter.txt:\\nYOU ARE THE BEST'}]\n",
      "=======================\n",
      "Tool Use:\n",
      "\tToolUseId:  tooluse_nRdajCV2SieeqQWyp-v5Sw\n",
      "\tname:  word_count\n",
      "\tinput:  {'text': 'YOU ARE THE BEST'}\n",
      "Tool Result:\n",
      "\tToolUseId:  tooluse_nRdajCV2SieeqQWyp-v5Sw\n",
      "\tStatus:  success\n",
      "\tContent:  [{'text': '4'}]\n",
      "=======================\n",
      "Tool Use:\n",
      "\tToolUseId:  tooluse_QqicIo7PTHy1iL8bJFfrWw\n",
      "\tname:  file_read\n",
      "\tinput:  {'path': 'letter.txt', 'mode': 'view'}\n",
      "Tool Result:\n",
      "\tToolUseId:  tooluse_QqicIo7PTHy1iL8bJFfrWw\n",
      "\tStatus:  success\n",
      "\tContent:  [{'text': 'Content of letter.txt:\\nYOU ARE THE BEST'}]\n",
      "=======================\n",
      "Tool Use:\n",
      "\tToolUseId:  tooluse_UsAow-quSCuUe3kRTKYikg\n",
      "\tname:  word_count\n",
      "\tinput:  {'text': 'YOU ARE THE BEST'}\n",
      "Tool Result:\n",
      "\tToolUseId:  tooluse_UsAow-quSCuUe3kRTKYikg\n",
      "\tStatus:  success\n",
      "\tContent:  [{'text': '4'}]\n",
      "=======================\n"
     ]
    }
   ],
   "source": [
    "for m in agent.messages:\n",
    "    for content in m[\"content\"]:\n",
    "        if \"toolUse\" in content:\n",
    "            print(\"Tool Use:\")\n",
    "            tool_use = content[\"toolUse\"]\n",
    "            print(\"\\tToolUseId: \", tool_use[\"toolUseId\"])\n",
    "            print(\"\\tname: \", tool_use[\"name\"])\n",
    "            print(\"\\tinput: \", tool_use[\"input\"])\n",
    "        if \"toolResult\" in content:\n",
    "            print(\"Tool Result:\")\n",
    "            tool_result = m[\"content\"][0][\"toolResult\"]\n",
    "            print(\"\\tToolUseId: \", tool_result[\"toolUseId\"])\n",
    "            print(\"\\tStatus: \", tool_result[\"status\"])\n",
    "            print(\"\\tContent: \", tool_result[\"content\"])\n",
    "            print(\"=======================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use DeepEval for tool use evaluation\n",
    "\n",
    "Now, we have use Strands Agents to build an assistant agent with five tools. First we use DeepEval for tool use evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tool correctness**\n",
    "\n",
    "\n",
    "Tool Correctness assesses whether an agent‚Äôs tool-calling behavior aligns with expectations by verifying that all required tools were correctly called. Unlike most LLM evaluation metrics, the Tool Correctness metric is a deterministic measure and not an LLM-judge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from deepeval.models.llms.amazon_bedrock_model import AmazonBedrockModel\n",
    "import nest_asyncio\n",
    "\n",
    "# Apply nest_asyncio at the start\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Initialize the Bedrock model (e.g., Claude)\n",
    "model = AmazonBedrockModel(\n",
    "    model_id=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "    region_name=\"us-east-1\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created ToolCall: {'name': 'file_read', 'description': 'Tool used in the conversation: file_read', 'reasoning': None, 'output': [\"{'text': 'Content of letter.txt:\\\\nYOU ARE THE BEST'}\"], 'input_parameters': {'path': 'letter.txt', 'mode': 'view'}}\n",
      "Created ToolCall: {'name': 'word_count', 'description': 'Tool used in the conversation: word_count', 'reasoning': None, 'output': [\"{'text': '4'}\"], 'input_parameters': {'text': 'YOU ARE THE BEST'}}\n",
      "Created ToolCall: {'name': 'file_read', 'description': 'Tool used in the conversation: file_read', 'reasoning': None, 'output': [\"{'text': 'Content of letter.txt:\\\\nYOU ARE THE BEST'}\"], 'input_parameters': {'path': 'letter.txt', 'mode': 'view'}}\n",
      "Created ToolCall: {'name': 'word_count', 'description': 'Tool used in the conversation: word_count', 'reasoning': None, 'output': [\"{'text': '4'}\"], 'input_parameters': {'text': 'YOU ARE THE BEST'}}\n",
      "Created tool calls:\n",
      "  - ToolCall: file_read\n",
      "  - ToolCall: word_count\n",
      "  - ToolCall: file_read\n",
      "  - ToolCall: word_count\n",
      "\n",
      "Debug - Tool calls: [ToolCall(\n",
      "    name=\"file_read\",\n",
      "    description=\"Tool used in the conversation: file_read\",\n",
      "    input_parameters={\n",
      "        \"path\": \"letter.txt\",\n",
      "        \"mode\": \"view\"\n",
      "    },\n",
      "    output=[\"{'text': 'Content of letter.txt:\\\\nYOU ARE THE BEST'}\"]\n",
      "), ToolCall(\n",
      "    name=\"word_count\",\n",
      "    description=\"Tool used in the conversation: word_count\",\n",
      "    input_parameters={\n",
      "        \"text\": \"YOU ARE THE BEST\"\n",
      "    },\n",
      "    output=[\"{'text': '4'}\"]\n",
      "), ToolCall(\n",
      "    name=\"file_read\",\n",
      "    description=\"Tool used in the conversation: file_read\",\n",
      "    input_parameters={\n",
      "        \"path\": \"letter.txt\",\n",
      "        \"mode\": \"view\"\n",
      "    },\n",
      "    output=[\"{'text': 'Content of letter.txt:\\\\nYOU ARE THE BEST'}\"]\n",
      "), ToolCall(\n",
      "    name=\"word_count\",\n",
      "    description=\"Tool used in the conversation: word_count\",\n",
      "    input_parameters={\n",
      "        \"text\": \"YOU ARE THE BEST\"\n",
      "    },\n",
      "    output=[\"{'text': '4'}\"]\n",
      ")]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Tool Correctness Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using </span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">None</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mTool Correctness Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing \u001b[0m\u001b[3;38;2;55;65;81mNone\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 1 test case(s) in parallel: |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|100% (1/1) [Time Taken: 00:00, 139.99test case/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚úÖ Tool Correctness (score: 1.0, threshold: 0.5, strict: False, evaluation model: None, reason: All expected tools ['file_read', 'word_count'] were called (order not considered)., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: You are a helpful assistant that provides concise responses. Help me to read letter.txt file, count the total number of words.\n",
      "  - actual output: The file contains 4 words: YOU, ARE, THE, BEST\n",
      "  - expected output: None\n",
      "  - context: None\n",
      "  - retrieval context: None\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Tool Correctness: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #05f58d; text-decoration-color: #05f58d\">‚úì</span> Tests finished üéâ! Run <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'deepeval login'</span> to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "‚ú®üëÄ Looking for a place for your LLM test data to live üè°‚ù§Ô∏è ? Use <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Confident AI</span> to get &amp; share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run <span style=\"color: #008080; text-decoration-color: #008080\">'deepeval login'</span> in the CLI. \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[38;2;5;245;141m‚úì\u001b[0m Tests finished üéâ! Run \u001b[1;32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "‚ú®üëÄ Looking for a place for your LLM test data to live üè°‚ù§Ô∏è ? Use \u001b[38;2;106;0;255mConfident AI\u001b[0m to get & share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run \u001b[36m'deepeval login'\u001b[0m in the CLI. \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "EvaluationResult(test_results=[TestResult(name='test_case_0', success=True, metrics_data=[MetricData(name='Tool Correctness', threshold=0.5, success=True, score=1.0, reason=\"All expected tools ['file_read', 'word_count'] were called (order not considered).\", strict_mode=False, evaluation_model=None, error=None, evaluation_cost=None, verbose_logs='Expected Tools:\\n[\\n    ToolCall(\\n        name=\"file_read\"\\n    ),\\n    ToolCall(\\n        name=\"word_count\"\\n    )\\n] \\n \\nTools Called:\\n[\\n    ToolCall(\\n        name=\"file_read\",\\n        description=\"Tool used in the conversation: file_read\",\\n        input_parameters={\\n            \"path\": \"letter.txt\",\\n            \"mode\": \"view\"\\n        },\\n        output=[\"{\\'text\\': \\'Content of letter.txt:\\\\\\\\nYOU ARE THE BEST\\'}\"]\\n    ),\\n    ToolCall(\\n        name=\"word_count\",\\n        description=\"Tool used in the conversation: word_count\",\\n        input_parameters={\\n            \"text\": \"YOU ARE THE BEST\"\\n        },\\n        output=[\"{\\'text\\': \\'4\\'}\"]\\n    ),\\n    ToolCall(\\n        name=\"file_read\",\\n        description=\"Tool used in the conversation: file_read\",\\n        input_parameters={\\n            \"path\": \"letter.txt\",\\n            \"mode\": \"view\"\\n        },\\n        output=[\"{\\'text\\': \\'Content of letter.txt:\\\\\\\\nYOU ARE THE BEST\\'}\"]\\n    ),\\n    ToolCall(\\n        name=\"word_count\",\\n        description=\"Tool used in the conversation: word_count\",\\n        input_parameters={\\n            \"text\": \"YOU ARE THE BEST\"\\n        },\\n        output=[\"{\\'text\\': \\'4\\'}\"]\\n    )\\n]')], conversational=False, multimodal=False, input='You are a helpful assistant that provides concise responses. Help me to read letter.txt file, count the total number of words.', actual_output='The file contains 4 words: YOU, ARE, THE, BEST', expected_output=None, context=None, retrieval_context=None, additional_metadata=None)], confident_link=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import HallucinationMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.metrics import ToolCorrectnessMetric\n",
    "from deepeval.test_case import LLMTestCase, ToolCall,ToolCallParams\n",
    "from deepeval import evaluate\n",
    "from extract_tool_calls import extract_tool_calls_from_strands\n",
    "\n",
    "\n",
    "\n",
    "tool_calls, final_text = extract_tool_calls_from_strands(agent.messages)\n",
    "\n",
    "# Debug tool calls\n",
    "print(\"\\nDebug - Tool calls:\", tool_calls)\n",
    "\n",
    "# Create test case with string output\n",
    "test_case = LLMTestCase(\n",
    "    input=message,\n",
    "    actual_output=\"The file contains 4 words: YOU, ARE, THE, BEST\",\n",
    "    tools_called=tool_calls,\n",
    "    expected_tools=[ToolCall(name=\"file_read\"), ToolCall(name=\"word_count\")]\n",
    ")\n",
    "\n",
    "task_Correctness_metric = ToolCorrectnessMetric()\n",
    "\n",
    "# Run evaluation synchronously\n",
    "evaluate(\n",
    "    test_cases=[test_case],\n",
    "    metrics=[task_Correctness_metric],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tool Efficiency**\n",
    "\n",
    "\n",
    "Equally important to tool correctness is tool efficiency. Inefficient tool-calling patterns can increase response times, frustrate users, and significantly raise operational costs.\n",
    "\n",
    "\n",
    "Let‚Äôs explore how tool efficiency can be evaluated:\n",
    "\n",
    "* Redundant Tool --  Usage measures how many tools are invoked unnecessarily ‚Äî those that do not directly contribute to achieving the intended outcome. This can be calculated as the percentage of unnecessary tools relative to the total number of tool invocations.\n",
    "* Tool Frequency -- evaluates whether tools are being called more often than necessary. This method penalizes tools that exceed a predefined threshold for the number of calls required to complete a task (many times this is just 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from deepeval.metrics import GEval\n",
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Tool Efficiency </span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">(</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">GEval</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">)</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\"> Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">us.anthropic.claude-</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">3</span><span style=\"color: #374151; text-decoration-color: #374151\">-</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">7</span><span style=\"color: #374151; text-decoration-color: #374151\">-sonnet-</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">20250219</span><span style=\"color: #374151; text-decoration-color: #374151\">-v</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">1:0</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mTool Efficiency \u001b[0m\u001b[1;38;2;106;0;255m(\u001b[0m\u001b[38;2;106;0;255mGEval\u001b[0m\u001b[1;38;2;106;0;255m)\u001b[0m\u001b[38;2;106;0;255m Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing \u001b[0m\n",
       "\u001b[38;2;55;65;81mus.anthropic.claude-\u001b[0m\u001b[1;38;2;55;65;81m3\u001b[0m\u001b[38;2;55;65;81m-\u001b[0m\u001b[1;38;2;55;65;81m7\u001b[0m\u001b[38;2;55;65;81m-sonnet-\u001b[0m\u001b[1;38;2;55;65;81m20250219\u001b[0m\u001b[38;2;55;65;81m-v\u001b[0m\u001b[1;38;2;55;65;81m1:0\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 1 test case(s) in parallel: |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|100% (1/1) [Time Taken: 00:15, 15.52s/test case]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚ùå Tool Efficiency (GEval) (score: 0.4, threshold: 0.7, strict: False, evaluation model: us.anthropic.claude-3-7-sonnet-20250219-v1:0, reason: The assistant correctly used the necessary tools (file_read and word_count) to complete the task of counting words in letter.txt, and the output accurately reports 4 words. However, there are significant inefficiencies: both tools were called twice with identical parameters, producing duplicate results. The task could have been completed with just one file_read call followed by one word_count call, making half of the tool calls redundant., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: You are a helpful assistant that provides concise responses. Help me to read letter.txt file, count the total number of words.\n",
      "  - actual output: {'text': 'The letter.txt file contains 4 words.'}\n",
      "  - expected output: None\n",
      "  - context: None\n",
      "  - retrieval context: None\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Tool Efficiency (GEval): 0.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #05f58d; text-decoration-color: #05f58d\">‚úì</span> Tests finished üéâ! Run <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'deepeval login'</span> to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "‚ú®üëÄ Looking for a place for your LLM test data to live üè°‚ù§Ô∏è ? Use <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Confident AI</span> to get &amp; share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run <span style=\"color: #008080; text-decoration-color: #008080\">'deepeval login'</span> in the CLI. \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[38;2;5;245;141m‚úì\u001b[0m Tests finished üéâ! Run \u001b[1;32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "‚ú®üëÄ Looking for a place for your LLM test data to live üè°‚ù§Ô∏è ? Use \u001b[38;2;106;0;255mConfident AI\u001b[0m to get & share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run \u001b[36m'deepeval login'\u001b[0m in the CLI. \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "EvaluationResult(test_results=[TestResult(name='test_case_0', success=False, metrics_data=[MetricData(name='Tool Efficiency (GEval)', threshold=0.7, success=False, score=0.4, reason='The assistant correctly used the necessary tools (file_read and word_count) to complete the task of counting words in letter.txt, and the output accurately reports 4 words. However, there are significant inefficiencies: both tools were called twice with identical parameters, producing duplicate results. The task could have been completed with just one file_read call followed by one word_count call, making half of the tool calls redundant.', strict_mode=False, evaluation_model='us.anthropic.claude-3-7-sonnet-20250219-v1:0', error=None, evaluation_cost=0.0, verbose_logs='Criteria:\\n\\nDetermine whether the tool effectively be used.\\nRedundant Tool Usage measures how many tools are invoked unnecessarily ‚Äî those that do not directly contribute to achieving the intended outcome.\\nTool Frequency evaluates whether tools are being called more often than necessary.\\n \\n \\nEvaluation Steps:\\n[\\n    \"Check if the Input clearly defines a task that requires tool usage, and verify if the Actual Output addresses this task effectively\",\\n    \"Examine Tools Called to identify any redundant tools that don\\'t contribute to the final solution in the Actual Output\",\\n    \"Compare the number of tool calls against the minimum necessary to achieve the Actual Output\",\\n    \"Assess whether multiple tool calls could have been consolidated into fewer calls while still producing the same Actual Output\"\\n] \\n \\nRubric:\\nNone')], conversational=False, multimodal=False, input='You are a helpful assistant that provides concise responses. Help me to read letter.txt file, count the total number of words.', actual_output=\"{'text': 'The letter.txt file contains 4 words.'}\", expected_output=None, context=None, retrieval_context=None, additional_metadata=None)], confident_link=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Create test case\n",
    "test_case = LLMTestCase(\n",
    "    input=message,\n",
    "    actual_output=final_text,\n",
    "    tools_called=tool_calls\n",
    ")\n",
    "\n",
    "\n",
    "# Create G-Eval metric for tool efficiency with Bedrock model\n",
    "g_eval_metric = GEval(\n",
    "    name=\"Tool Efficiency\",\n",
    "    criteria=\"\"\"\n",
    "Determine whether the tool effectively be used.\n",
    "Redundant Tool Usage measures how many tools are invoked unnecessarily ‚Äî those that do not directly contribute to achieving the intended outcome.\n",
    "Tool Frequency evaluates whether tools are being called more often than necessary.\n",
    "\"\"\",\n",
    "    threshold=0.7,  # Set a reasonable threshold for tool efficiency\n",
    "    evaluation_params=[\n",
    "        LLMTestCaseParams.INPUT,\n",
    "        LLMTestCaseParams.ACTUAL_OUTPUT,\n",
    "        LLMTestCaseParams.TOOLS_CALLED\n",
    "    ],\n",
    "    model=model  # Explicitly pass the Bedrock model\n",
    ")\n",
    "\n",
    "# Run evaluation synchronously\n",
    "evaluate(\n",
    "    test_cases=[test_case],\n",
    "    metrics=[g_eval_metric],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Agentic Workflows\n",
    "\n",
    "**Task Completion**\n",
    "\n",
    "A critical metric for assessing agent workflows is Task Completion (also known as task success or goal accuracy). This metric measures how effectively an LLM agent completes a user-given task. \n",
    "\n",
    "However, in real-world applications, agents are often required to perform a diverse set of tasks‚Äîmany of which may lack predefined ground-truth datasets.DeepEval‚Äôs Task Completion metric addresses these challenges by leveraging LLMs to:\n",
    "\n",
    "* Determine the task from the user‚Äôs input.\n",
    "* Analyze the reasoning steps, tool usage, and final response to assess whether the task was successfully completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Task Completion Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using us.anthropic.claude-</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">3</span><span style=\"color: #374151; text-decoration-color: #374151\">-</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">7</span><span style=\"color: #374151; text-decoration-color: #374151\">-sonnet-</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">20250219</span><span style=\"color: #374151; text-decoration-color: #374151\">-v</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">1:0</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mTask Completion Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing us.anthropic.claude-\u001b[0m\u001b[1;38;2;55;65;81m3\u001b[0m\u001b[38;2;55;65;81m-\u001b[0m\u001b[1;38;2;55;65;81m7\u001b[0m\u001b[38;2;55;65;81m-sonnet-\u001b[0m\u001b[1;38;2;55;65;81m20250219\u001b[0m\u001b[38;2;55;65;81m-v\u001b[0m\u001b[1;38;2;55;65;81m1:0\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\n",
       "\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 1 test case(s) in parallel: |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|100% (1/1) [Time Taken: 00:05,  5.22s/test case]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚úÖ Task Completion (score: 1.0, threshold: 0.5, strict: False, evaluation model: us.anthropic.claude-3-7-sonnet-20250219-v1:0, reason: The actual outcome perfectly achieves the user's goal. The system successfully read the letter.txt file and accurately counted the total number of words (4) in the file content 'YOU ARE THE BEST'., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: You are a helpful assistant that provides concise responses. Help me to read letter.txt file, count the total number of words.\n",
      "  - actual output: {'text': 'The letter.txt file contains 4 words.'}\n",
      "  - expected output: None\n",
      "  - context: None\n",
      "  - retrieval context: None\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Task Completion: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #05f58d; text-decoration-color: #05f58d\">‚úì</span> Tests finished üéâ! Run <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'deepeval login'</span> to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "‚ú®üëÄ Looking for a place for your LLM test data to live üè°‚ù§Ô∏è ? Use <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Confident AI</span> to get &amp; share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run <span style=\"color: #008080; text-decoration-color: #008080\">'deepeval login'</span> in the CLI. \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[38;2;5;245;141m‚úì\u001b[0m Tests finished üéâ! Run \u001b[1;32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "‚ú®üëÄ Looking for a place for your LLM test data to live üè°‚ù§Ô∏è ? Use \u001b[38;2;106;0;255mConfident AI\u001b[0m to get & share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run \u001b[36m'deepeval login'\u001b[0m in the CLI. \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "EvaluationResult(test_results=[TestResult(name='test_case_0', success=True, metrics_data=[MetricData(name='Task Completion', threshold=0.5, success=True, score=1.0, reason=\"The actual outcome perfectly achieves the user's goal. The system successfully read the letter.txt file and accurately counted the total number of words (4) in the file content 'YOU ARE THE BEST'.\", strict_mode=False, evaluation_model='us.anthropic.claude-3-7-sonnet-20250219-v1:0', error=None, evaluation_cost=0.0, verbose_logs=\"User Goal: Read letter.txt file and count the total number of words. \\n \\nTask Outcome: The system read the letter.txt file which contained 'YOU ARE THE BEST' and counted 4 words in the file.\")], conversational=False, multimodal=False, input='You are a helpful assistant that provides concise responses. Help me to read letter.txt file, count the total number of words.', actual_output=\"{'text': 'The letter.txt file contains 4 words.'}\", expected_output=None, context=None, retrieval_context=None, additional_metadata=None)], confident_link=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepeval.metrics import TaskCompletionMetric\n",
    "\n",
    "# Create test case\n",
    "test_case = LLMTestCase(\n",
    "    input=message,\n",
    "    actual_output=final_text,\n",
    "    tools_called=tool_calls\n",
    ")\n",
    "\n",
    "\n",
    "task_completion_metric = TaskCompletionMetric(model=model)\n",
    "\n",
    "# Run evaluation synchronously\n",
    "evaluate(\n",
    "    test_cases=[test_case],\n",
    "    metrics=[task_completion_metric],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
