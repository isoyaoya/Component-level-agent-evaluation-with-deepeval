{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using DeepEval for AWS Bedrock LLM evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using DeepEval pre-build metrics for popular tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hallucination Detection\n",
    "There are some papers that specifically research how to detect hallucinations based on LLM as a judge. For example, in the FACTOOL method(https://arxiv.org/pdf/2307.13528), the author proposes to first extract the claims that can be judged from the LLM's response, then generate queryable questions based on each claim, and then query on Google search or a customized knowledge base to obtain the evidence corresponding to each claim. Finally, based on the evidence, it is determined whether each claim is consistent with the factual basis, so as to determine whether there is hallucination in the answer generated by the model. \n",
    "\n",
    "Deepeval uses a simpler but similar pipeline for hallucination detection. To use the HallucinationMetric, you'll have to provide the following arguments:\n",
    "\n",
    "* input: Means input prompt\n",
    "* actual_output: Output from LLM\n",
    "* context: Evidence or retrieved knowledge, could be a list\n",
    "\n",
    "Then, the hallucination metric uses LLM-as-a-judge to determine whether your LLM generates factually correct information by comparing the actual_output to the provided context and output a score that can be calculated according to the following equation:\n",
    "\n",
    "$$ \\text{Hallucination} = \\frac{\\text{Number of Contradicted Contexts}}{\\text{Total Number of Contexts}} $$\n",
    "\n",
    "You can find more details about how DeepEval HallucinationMetric works from here:\n",
    "https://github.com/confident-ai/deepeval/blob/main/deepeval/metrics/hallucination/template.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install aiobotocore botocore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from deepeval.models.llms.amazon_bedrock_model import AmazonBedrockModel\n",
    "import nest_asyncio\n",
    "\n",
    "# Apply nest_asyncio at the start\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Initialize the Bedrock model (e.g., Claude)\n",
    "model = AmazonBedrockModel(\n",
    "    model_id=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "    region_name=\"us-east-1\"\n",
    ")\n",
    "\n",
    "# Define your input prompt\n",
    "prompt1 = '''What new reasoning feature does Claude 3.7 Sonnet introduce?'''\n",
    "\n",
    "# Run the model\n",
    "output = model.generate(prompt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 The score is 1.00 because there are no factual alignments and the output completely contradicts the context by incorrectly naming Claude 3.7 Sonnet's reasoning feature as 'chain-of-thought reasoning with self-critique' instead of 'extended thinking' as stated in the context, and by omitting the important detail that this feature is toggleable by users.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Hallucination Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using us.anthropic.claude-</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">3</span><span style=\"color: #374151; text-decoration-color: #374151\">-</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">7</span><span style=\"color: #374151; text-decoration-color: #374151\">-sonnet-</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">20250219</span><span style=\"color: #374151; text-decoration-color: #374151\">-v</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">1:0</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mHallucination Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing us.anthropic.claude-\u001b[0m\u001b[1;38;2;55;65;81m3\u001b[0m\u001b[38;2;55;65;81m-\u001b[0m\u001b[1;38;2;55;65;81m7\u001b[0m\u001b[38;2;55;65;81m-sonnet-\u001b[0m\u001b[1;38;2;55;65;81m20250219\u001b[0m\u001b[38;2;55;65;81m-v\u001b[0m\u001b[1;38;2;55;65;81m1:0\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\n",
       "\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 1 test case(s) in parallel: |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|100% (1/1) [Time Taken: 00:06,  6.98s/test case]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚ùå Hallucination (score: 1.0, threshold: 0.5, strict: False, evaluation model: us.anthropic.claude-3-7-sonnet-20250219-v1:0, reason: The score is 1.00 because the output contains a direct contradiction with the context, incorrectly referring to Claude 3.7 Sonnet's feature as 'chain-of-thought reasoning with self-critique' when the context specifically names it 'extended thinking'. With no factual alignments and a clear contradiction about a key feature name, the output is completely hallucinated., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What new reasoning feature does Claude 3.7 Sonnet introduce?\n",
      "  - actual output: ('Claude 3.7 Sonnet introduces a new reasoning feature called \"chain-of-thought reasoning with self-critique.\" This capability allows me to:\\n\\n1. Break down complex problems into steps\\n2. Evaluate my own reasoning as I go\\n3. Identify and correct potential errors in my thinking\\n4. Consider alternative approaches when appropriate\\n\\nThis self-critique mechanism helps me produce more reliable answers by catching flaws in my reasoning process before finalizing my response. It\\'s particularly valuable for tasks requiring careful logical analysis, mathematical problem-solving, or nuanced judgment.', 0)\n",
      "  - expected output: None\n",
      "  - context: ['Anthropic Claude 3.7 Sonnet is the first Claude model to introduce optional step-by-step reasoning, called \"extended thinking,\" which users can toggle alongside standard thinking. It supports up to 128K output tokens per request (with 64K‚Äì128K currently in beta) and features an enhanced computer use beta with support for new automated actions.']\n",
      "  - retrieval context: None\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Hallucination: 0.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #05f58d; text-decoration-color: #05f58d\">‚úì</span> Tests finished üéâ! Run <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'deepeval login'</span> to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "‚ú®üëÄ Looking for a place for your LLM test data to live üè°‚ù§Ô∏è ? Use <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Confident AI</span> to get &amp; share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run <span style=\"color: #008080; text-decoration-color: #008080\">'deepeval login'</span> in the CLI. \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[38;2;5;245;141m‚úì\u001b[0m Tests finished üéâ! Run \u001b[1;32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "‚ú®üëÄ Looking for a place for your LLM test data to live üè°‚ù§Ô∏è ? Use \u001b[38;2;106;0;255mConfident AI\u001b[0m to get & share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run \u001b[36m'deepeval login'\u001b[0m in the CLI. \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "EvaluationResult(test_results=[TestResult(name='test_case_0', success=False, metrics_data=[MetricData(name='Hallucination', threshold=0.5, success=False, score=1.0, reason=\"The score is 1.00 because the output contains a direct contradiction with the context, incorrectly referring to Claude 3.7 Sonnet's feature as 'chain-of-thought reasoning with self-critique' when the context specifically names it 'extended thinking'. With no factual alignments and a clear contradiction about a key feature name, the output is completely hallucinated.\", strict_mode=False, evaluation_model='us.anthropic.claude-3-7-sonnet-20250219-v1:0', error=None, evaluation_cost=0.0, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"The actual output contradicts the provided context. The context states that Claude 3.7 Sonnet introduces \\'optional step-by-step reasoning, called \\\\\"extended thinking\\\\\"\\', but the output incorrectly refers to this feature as \\'chain-of-thought reasoning with self-critique\\'. The feature name is incorrect in the actual output.\"\\n    }\\n]')], conversational=False, multimodal=False, input='What new reasoning feature does Claude 3.7 Sonnet introduce?', actual_output='(\\'Claude 3.7 Sonnet introduces a new reasoning feature called \"chain-of-thought reasoning with self-critique.\" This capability allows me to:\\\\n\\\\n1. Break down complex problems into steps\\\\n2. Evaluate my own reasoning as I go\\\\n3. Identify and correct potential errors in my thinking\\\\n4. Consider alternative approaches when appropriate\\\\n\\\\nThis self-critique mechanism helps me produce more reliable answers by catching flaws in my reasoning process before finalizing my response. It\\\\\\'s particularly valuable for tasks requiring careful logical analysis, mathematical problem-solving, or nuanced judgment.\\', 0)', expected_output=None, context=['Anthropic Claude 3.7 Sonnet is the first Claude model to introduce optional step-by-step reasoning, called \"extended thinking,\" which users can toggle alongside standard thinking. It supports up to 128K output tokens per request (with 64K‚Äì128K currently in beta) and features an enhanced computer use beta with support for new automated actions.'], retrieval_context=None, additional_metadata=None)], confident_link=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import HallucinationMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "retrieval_context = [\n",
    "    '''Anthropic Claude 3.7 Sonnet is the first Claude model to introduce optional step-by-step reasoning, called \"extended thinking,\" which users can toggle alongside standard thinking. '''\n",
    "    '''It supports up to 128K output tokens per request (with 64K‚Äì128K currently in beta) and features an enhanced computer use beta with support for new automated actions.'''\n",
    "]\n",
    "\n",
    "# Convert output to string if it's not already\n",
    "actual_output = str(output) if not isinstance(output, str) else output\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "    input=prompt1,\n",
    "    actual_output=actual_output,\n",
    "    context=retrieval_context\n",
    ")\n",
    "metric = HallucinationMetric(model=model)\n",
    "\n",
    "# To run metric as a standalone\n",
    "metric.measure(test_case)\n",
    "print(metric.score, metric.reason)\n",
    "\n",
    "evaluate(test_cases=[test_case], metrics=[metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prompt alignment metric uses LLM-as-a-judge to measure whether your LLM application is able to generate actual_outputs that aligns with any instructions specified in your prompt template. The algorithm is simple yet effective:\n",
    "* (1) Loop through all instructions found in your prompt template, before...\n",
    "* (2) Determining whether each instruction is followed based on the input and output\n",
    "\n",
    "This works because instead of supplying the entire prompt to the metric, we only supply the list of instructions, which means your judge LLM instead of having to take in the entire prompt as context (which can be lengthy and cause hallucinations), it just has to consider one instruction at a time when making a verdict on whether an instruction is followed.\n",
    "\n",
    "The score can be calculated according to the following equation:\n",
    "\n",
    "$$ \\text{Prompt Alignment} = \\frac{\\text{Number of Instructions Followed}}{\\text{Total¬†Number¬†of¬†Instructions}} $$\n",
    "\n",
    "You can find more details about how DeepEval prompt alignment metric works from here:\n",
    "https://github.com/confident-ai/deepeval/blob/main/deepeval/metrics/prompt_alignment/template.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define your input prompt\n",
    "prompt2 = '''Replace the lowwercase to uppercase, just output results: HelLo BedROck'''\n",
    "\n",
    "# Run the model\n",
    "output = model.generate(prompt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Prompt Alignment Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using us.anthropic.claude-</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">3</span><span style=\"color: #374151; text-decoration-color: #374151\">-</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">7</span><span style=\"color: #374151; text-decoration-color: #374151\">-sonnet-</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">20250219</span><span style=\"color: #374151; text-decoration-color: #374151\">-v</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">1:0</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mPrompt Alignment Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing us.anthropic.claude-\u001b[0m\u001b[1;38;2;55;65;81m3\u001b[0m\u001b[38;2;55;65;81m-\u001b[0m\u001b[1;38;2;55;65;81m7\u001b[0m\u001b[38;2;55;65;81m-sonnet-\u001b[0m\u001b[1;38;2;55;65;81m20250219\u001b[0m\u001b[38;2;55;65;81m-v\u001b[0m\u001b[1;38;2;55;65;81m1:0\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\n",
       "\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 1 test case(s) in parallel: |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|100% (1/1) [Time Taken: 00:07,  7.19s/test case]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚úÖ Prompt Alignment (score: 1.0, threshold: 0.5, strict: False, evaluation model: us.anthropic.claude-3-7-sonnet-20250219-v1:0, reason: The score is 1.00 because the LLM perfectly followed the instruction to replace lowercase letters with uppercase ones in 'HelLo BedROck', correctly outputting 'HELLO BEDROCK'. Great job on achieving perfect alignment with the prompt requirements!, error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Replace the lowwercase to uppercase, just output results: HelLo BedROck\n",
      "  - actual output: ('HELLO BEDROCK', 0)\n",
      "  - expected output: None\n",
      "  - context: None\n",
      "  - retrieval context: None\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Prompt Alignment: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #05f58d; text-decoration-color: #05f58d\">‚úì</span> Tests finished üéâ! Run <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'deepeval login'</span> to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "‚ú®üëÄ Looking for a place for your LLM test data to live üè°‚ù§Ô∏è ? Use <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Confident AI</span> to get &amp; share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run <span style=\"color: #008080; text-decoration-color: #008080\">'deepeval login'</span> in the CLI. \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[38;2;5;245;141m‚úì\u001b[0m Tests finished üéâ! Run \u001b[1;32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "‚ú®üëÄ Looking for a place for your LLM test data to live üè°‚ù§Ô∏è ? Use \u001b[38;2;106;0;255mConfident AI\u001b[0m to get & share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run \u001b[36m'deepeval login'\u001b[0m in the CLI. \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "EvaluationResult(test_results=[TestResult(name='test_case_0', success=True, metrics_data=[MetricData(name='Prompt Alignment', threshold=0.5, success=True, score=1.0, reason=\"The score is 1.00 because the LLM perfectly followed the instruction to replace lowercase letters with uppercase ones in 'HelLo BedROck', correctly outputting 'HELLO BEDROCK'. Great job on achieving perfect alignment with the prompt requirements!\", strict_mode=False, evaluation_model='us.anthropic.claude-3-7-sonnet-20250219-v1:0', error=None, evaluation_cost=0.0, verbose_logs='Prompt Instructions:\\n[\\n    \"Reply in all uppercase\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    }\\n]')], conversational=False, multimodal=False, input='Replace the lowwercase to uppercase, just output results: HelLo BedROck', actual_output=\"('HELLO BEDROCK', 0)\", expected_output=None, context=None, retrieval_context=None, additional_metadata=None)], confident_link=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.metrics import PromptAlignmentMetric\n",
    "\n",
    "metric = PromptAlignmentMetric(\n",
    "    prompt_instructions=[\"Reply in all uppercase\"],\n",
    "    model=model,\n",
    "    include_reason=True\n",
    ")\n",
    "test_case = LLMTestCase(\n",
    "    input=prompt2,\n",
    "    # Replace this with the actual output from your LLM application\n",
    "    actual_output=str(output)\n",
    ")\n",
    "\n",
    "\n",
    "evaluate(test_cases=[test_case], metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G-Eval for customized task-specific LLM Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "G-Eval represents a contemporary evaluation methodology introduced in the research paper \"NLG Evaluation using GPT-4 with Better Human Alignment\" (available at https://arxiv.org/pdf/2303.16634). This framework employs large language models to assess outputs from LLMs (commonly referred to as LLM-Evals) and stands as one of the premier approaches for developing bespoke, task-oriented assessment metrics. The overall framework of G-EVAL:\n",
    "\n",
    "\n",
    "Initially, it feeds **Task Introduction** and **Evaluation Criteria** into the language model, prompting it to create a Chain-of-Thought comprising detailed evaluation steps. Subsequently, prompt together with the generated Chain-of-Thought, is utilized to evaluate LLM outputs through a form-completion paradigm. The final assessment score is calculated via probability-weighted aggregation of the output scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.models.llms.amazon_bedrock_model import AmazonBedrockModel\n",
    "import nest_asyncio\n",
    "\n",
    "# Apply nest_asyncio at the start\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Initialize the Bedrock model (e.g., Claude)\n",
    "model = AmazonBedrockModel(\n",
    "    model_id=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "    region_name=\"us-east-1\"\n",
    ")\n",
    "\n",
    "# Define your input prompt\n",
    "prompt1 = '''Describe how technological innovations have transformed education over the past century. Analyze both the benefits and drawbacks of these changes, and predict how emerging technologies might further change educational practices in the next decade.'''\n",
    "\n",
    "# Run the model\n",
    "output = model.generate(prompt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
    "from deepeval.metrics import GEval\n",
    "\n",
    "test_case = LLMTestCase(input=prompt1, actual_output=output)\n",
    "coherence_metric = GEval(\n",
    "    name=\"Coherence\",\n",
    "    criteria=\"Coherence - the collective quality of all sentences in the actual output\",\n",
    "    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    model=model\n",
    ")\n",
    "\n",
    "coherence_metric.measure(test_case)\n",
    "print(coherence_metric.score)\n",
    "print(coherence_metric.reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
